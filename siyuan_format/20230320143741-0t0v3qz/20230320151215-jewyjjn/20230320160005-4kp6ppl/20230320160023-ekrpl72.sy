{
	"ID": "20230320160023-ekrpl72",
	"Spec": "1",
	"Type": "NodeDocument",
	"Properties": {
		"id": "20230320160023-ekrpl72",
		"scroll": "{\u0026quot;startId\u0026quot;:\u0026quot;20230321154822-be9xdxk\u0026quot;,\u0026quot;endId\u0026quot;:\u0026quot;20230320160023-cnb3zd4\u0026quot;,\u0026quot;scrollTop\u0026quot;:0,\u0026quot;focusId\u0026quot;:\u0026quot;20230321154822-1xk1t5i\u0026quot;,\u0026quot;focusStart\u0026quot;:0,\u0026quot;focusEnd\u0026quot;:0}",
		"title": "Status Compute_Xprop",
		"updated": "20230321154947"
	},
	"Children": [
		{
			"ID": "20230321154822-be9xdxk",
			"Type": "NodeParagraph",
			"Properties": {
				"id": "20230321154822-be9xdxk",
				"updated": "20230321154822"
			},
			"Children": [
				{
					"Type": "NodeText",
					"Data": "确定各种参数...在此过程用调用"
				},
				{
					"Type": "NodeTextMark",
					"TextMarkType": "block-ref",
					"TextMarkBlockRefID": "20230320150954-3zfqhco",
					"TextMarkBlockRefSubtype": "s",
					"TextMarkTextContent": "ClosestDivisorTo4"
				},
				{
					"Type": "NodeText",
					"Data": "\n最终计算时，确定是否有tensorcores"
				}
			]
		},
		{
			"ID": "20230321154822-467gy0u",
			"Type": "NodeSuperBlock",
			"Properties": {
				"id": "20230321154822-467gy0u",
				"updated": "20230321154947"
			},
			"Children": [
				{
					"Type": "NodeSuperBlockOpenMarker"
				},
				{
					"Type": "NodeSuperBlockLayoutMarker",
					"Data": "row"
				},
				{
					"ID": "20230321154822-hvpmbif",
					"Type": "NodeParagraph",
					"Properties": {
						"id": "20230321154822-hvpmbif",
						"updated": "20230321154822"
					},
					"Children": [
						{
							"Type": "NodeText",
							"Data": "参考如下判断流程根据tensorcores、axis_和blk的值来分别调用不同的核函数"
						}
					]
				},
				{
					"ID": "20230321154822-lwwnw4q",
					"Type": "NodeBlockquote",
					"Properties": {
						"id": "20230321154822-lwwnw4q",
						"updated": "20230321154822"
					},
					"Children": [
						{
							"Type": "NodeBlockquoteMarker",
							"Data": "\u003e"
						},
						{
							"ID": "20230321154822-0u5fnno",
							"Type": "NodeParagraph",
							"Properties": {
								"id": "20230321154822-0u5fnno",
								"updated": "20230321154822"
							},
							"Children": [
								{
									"Type": "NodeTextMark",
									"TextMarkType": "block-ref",
									"TextMarkBlockRefID": "20230320154446-lqknzrx",
									"TextMarkBlockRefSubtype": "s",
									"TextMarkTextContent": "hgemm_blocksparse_xn_64_sdd"
								},
								{
									"Type": "NodeText",
									"Data": "\n"
								},
								{
									"Type": "NodeTextMark",
									"TextMarkType": "block-ref",
									"TextMarkBlockRefID": "20230320154849-3k1b1o4",
									"TextMarkBlockRefSubtype": "s",
									"TextMarkTextContent": "hgemm_blocksparse_xn_128_sdd"
								},
								{
									"Type": "NodeText",
									"Data": "\n"
								},
								{
									"Type": "NodeTextMark",
									"TextMarkType": "block-ref",
									"TextMarkBlockRefID": "20230320154957-wmr2s1l",
									"TextMarkBlockRefSubtype": "s",
									"TextMarkTextContent": "hgemm_blocksparse_nx"
								},
								{
									"Type": "NodeTextMark",
									"TextMarkType": "block-ref",
									"TextMarkBlockRefID": "20230320153549-pahtfsp",
									"TextMarkBlockRefSubtype": "s",
									"TextMarkTextContent": "BsmmXprop_CN\nBsmmXprop_CN"
								}
							]
						}
					]
				},
				{
					"ID": "20230321154822-1xk1t5i",
					"Type": "NodeCodeBlock",
					"IsFencedCodeBlock": true,
					"Properties": {
						"fold": "0",
						"id": "20230321154822-1xk1t5i",
						"style": "height: 300px;",
						"updated": "20230321154947"
					},
					"Children": [
						{
							"Type": "NodeCodeBlockFenceOpenMarker",
							"Data": "```"
						},
						{
							"Type": "NodeCodeBlockFenceInfoMarker",
							"CodeBlockInfo": "bWluZG1hcA=="
						},
						{
							"Type": "NodeCodeBlockCode",
							"Data": "- 判断tensorcores\n  - 有tensorcores\n    - axis_ == 0\n      - blk == 64\n        - hgemm_blocksparse_xn_64_sdd\n      - blk != 64\n        - hgemm_blocksparse_xn_128_sdd\n    - axis_ != 0\n      - hgemm_blocksparse_nx_dsd\n  - 无tensorcores\n    - Gate == NULL且axis_ == 0\n      - op == FPROP_OP\n        - BsmmXprop_CN\u0026lt; true,NTYPE(T)\u0026gt;\n      - op != BPROP_OP\n        - BsmmXprop_CN\u0026lt;false,NTYPE(T)\u0026gt;\n    - else\n      - 报错(目前只支持fp16)"
						},
						{
							"Type": "NodeCodeBlockFenceCloseMarker",
							"Data": "```"
						}
					]
				},
				{
					"ID": "20230321154822-yryhkj1",
					"Type": "NodeCodeBlock",
					"IsFencedCodeBlock": true,
					"Properties": {
						"custom-riff-decks": "20230218211946-2kw8jgx",
						"id": "20230321154822-yryhkj1",
						"updated": "20230321154822"
					},
					"Children": [
						{
							"Type": "NodeCodeBlockFenceOpenMarker",
							"Data": "```"
						},
						{
							"Type": "NodeCodeBlockFenceInfoMarker",
							"CodeBlockInfo": "Y3Bw"
						},
						{
							"Type": "NodeCodeBlockCode",
							"Data": "/* tensorcores的判断\n判断当前 GPU 是否支持 Tensor Cores，并且 T1 是否是 ehalf 类型（即 NVIDIA Tensor Cores 支持的半精度浮点类型）。\n具体实现是通过检查当前 GPU 的主版本号是否大于等于 7 来判断是否支持 Tensor Cores。在 CUDA 9 中，NVIDIA 发布了 Volta 架构的 GPU，并加入了 Tensor Cores 的支持。这个架构的主版本号为 7，因此主版本号大于等于 7 的 GPU 都支持 Tensor Cores。\n另外，代码还使用了 std::is_same 模板来判断类型是否相同。在这里，T1 的类型是否是 ehalf，即是否是半精度浮点类型，也是判断是否支持 Tensor Cores 的条件之一。\n*/\nbool tensorcores = major_ \u003e= 7 \u0026\u0026 std::is_same\u003cT1, ehalf\u003e::value;\n"
						},
						{
							"Type": "NodeCodeBlockFenceCloseMarker",
							"Data": "```"
						}
					]
				},
				{
					"Type": "NodeSuperBlockCloseMarker"
				}
			]
		},
		{
			"ID": "20230320160023-cnb3zd4",
			"Type": "NodeCodeBlock",
			"IsFencedCodeBlock": true,
			"Properties": {
				"id": "20230320160023-cnb3zd4",
				"updated": "20230320160501"
			},
			"Children": [
				{
					"Type": "NodeCodeBlockFenceOpenMarker",
					"Data": "```"
				},
				{
					"Type": "NodeCodeBlockFenceInfoMarker",
					"CodeBlockInfo": "Y3Bw"
				},
				{
					"Type": "NodeCodeBlockCode",
					"Data": "Status Compute_Xprop(OpKernelContext* ctx, uint op)\n    {\n        const Tensor\u0026 A = ctx-\u003einput(0);\n        const Tensor\u0026 B = ctx-\u003einput(1);\n        const Tensor\u0026 L = ctx-\u003einput(2);\n\n        OpInputList gate;\n        ctx-\u003einput_list(\"gate\", \u0026gate);\n\n        TensorShape shapeC;\n        int N     = 1;\n        int rankA = A.dims();\n        for (int i = 0; i \u003c rankA; i++)\n            if (i != axis_)\n            {\n                shapeC.AddDim(A.dim_size(i));\n                N *= A.dim_size(i);\n            }\n            else\n                shapeC.AddDim(params_.K);\n\n        bool tensorcores = major_ \u003e= 7 \u0026\u0026 std::is_same\u003cT1, ehalf\u003e::value;\n\n        int blkN = 128, gridN = CEIL_DIV(N, 128), modN128 = N \u0026 127;\n        if (!tensorcores || axis_ == 1 || (modN128 \u003e 0 \u0026\u0026 modN128 \u003c= 64) || gridN * params_.segments \u003c SMs_*4)\n        {\n            blkN  = 64;\n            gridN = CEIL_DIV(N, 64);\n        }\n\n        Tensor* C;\n        Status s = ctx-\u003eallocate_output(0, shapeC, \u0026C);\n        if (!s.ok()) return s;\n\n        Tensor* Lock;\n        TensorShape shapeL;\n        if (params_.locks \u003e 0)\n            shapeL.AddDim(gridN * params_.locks * 2);\n        s = ctx-\u003eallocate_output(1, shapeL, \u0026Lock);\n        if (!s.ok()) return s;\n\n        params_.Lock = params_.locks \u003e 0 ? Lock-\u003eflat\u003cint32\u003e().data() : nullptr;\n        params_.N    = N;\n        params_.Lut  = (const int*)L.flat\u003cint64\u003e().data();\n        params_.Gate = gate.size() \u003e 0 ? gate[0].flat\u003cfloat\u003e().data() : NULL;\n\n        if (params_.blk_A == 0)\n        {\n            ClosestDivisorTo4(params_.segments, true, \u0026params_.blk_a, \u0026params_.blk_A);\n            ClosestDivisorTo4(gridN,           false, \u0026params_.blk_b, \u0026params_.blk_B);\n\n            // printf(\"%d %d %d %d %d %d\\n\", params_.segments, gridN, params_.blk_a, params_.blk_b, params_.blk_A, params_.blk_B);\n        }\n\n        const T1* pA = (const T1*)A.flat\u003cT\u003e().data();\n        const T1* pB = (const T1*)B.flat\u003cT\u003e().data();\n              T1* pC = (      T1*)C-\u003eflat\u003cT\u003e().data();\n\n        if (is_gpu_)\n            params_.stream = ((CUDAStream*)ctx-\u003eop_device_context()-\u003estream()-\u003eimplementation())-\u003ecuda_stream();\n\n        Benchmark* bench = nullptr;\n        if (bench_) bench = new Benchmark(params_.stream, bench_string_, 0, flops_ * params_.N * params_.pcount, repeat_, is_gpu_);\n\n        cudaError_t res;\n        for (int r = 0; r \u003c repeat_; r++)\n            if (tensorcores)\n            {\n                if (axis_ == 0)\n                    if (blkN == 64)\n                        res = hgemm_blocksparse_xn_64_sdd( pA, pB, pC, \u0026params_, op == FPROP_OP ? OP_T : OP_N);\n                    else\n                        res = hgemm_blocksparse_xn_128_sdd(pA, pB, pC, \u0026params_, op == FPROP_OP ? OP_T : OP_N);\n                else\n                    res = hgemm_blocksparse_nx_dsd(pA, pB, pC, \u0026params_, op == FPROP_OP ? OP_N : OP_T);\n            }\n            else\n            {\n                if (params_.Gate == NULL \u0026\u0026 axis_ == 0)\n                {\n                    if (op == FPROP_OP)\n                        res = BsmmXprop_CN\u003c true,NTYPE(T)\u003e(pA, pB, pC, \u0026params_);\n                    else\n                        res = BsmmXprop_CN\u003cfalse,NTYPE(T)\u003e(pA, pB, pC, \u0026params_);\n                }\n                else\n                {\n                    // Cuda update for Volta broke these kernels.  Need to fix.\n                    // Ideally merge gated and non-gated code like is done with hgemm kernels.\n                    return errors::Internal(\"Gated blocksparse matmul currently only supported on fp16 tensorcores.\");\n                    // if (op == NN_OP)\n                    //     res = BsmmGatedXprop_CN\u003cfalse,NTYPE(T)\u003e(pA, pB, pC, \u0026params_);\n                    // else\n                    //     res = BsmmGatedXprop_CN\u003c true,NTYPE(T)\u003e(pA, pB, pC, \u0026params_);\n                }\n            }\n\n        if (bench) delete bench;\n\n        if (cudaSuccess != res)\n            return errors::Internal(cudaGetErrorString(res));\n        return Status::OK();\n    }\n"
				},
				{
					"Type": "NodeCodeBlockFenceCloseMarker",
					"Data": "```"
				}
			]
		}
	]
}